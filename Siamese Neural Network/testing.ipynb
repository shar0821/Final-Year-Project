{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = tf.data.Dataset.list_files(ANC_PATH+'\\*.png').take(263)\n",
    "positive = tf.data.Dataset.list_files(POS_PATH+'\\*.png').take(263)\n",
    "negative = tf.data.Dataset.list_files(NEG_PATH+'\\*.png').take(263)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(file_path):\n",
    "#     byte_img = tf.io.read_file(file_path)\n",
    "#     img = tf.io.decode_png(byte_img)    \n",
    "#     # img = tf.image.resize(img, (128,64))\n",
    "#     img = img / 255\n",
    "#     img = img[:,:,0]\n",
    "#     return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def preprocess(file_path):\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_png(byte_img)    \n",
    "    # img = tf.image.resize(img, (128,64))\n",
    "    img = img / 255\n",
    "    img = img[:,:,0]\n",
    "    # print(type(img))\n",
    "    # img = tf.repeat([img], repeats=[3], axis=0)\n",
    "    # img = tf.transpose(img, perm=[1, 2, 0])\n",
    "    # img = img.transpose(2,0,1)\n",
    "    # print(tf.shape(img))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    return(preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "test_data = data.skip(round(len(data)*.7))\n",
    "test_data = test_data.take(round(len(data)*.3))\n",
    "print(len(test_data))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input, test_val, y_true = test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_input))\n",
    "print(len(test_val))\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distancing/differencing layer\n",
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('siamesemodel_vgg_custom_200+300+200+300_SGD_1e-2_m0.9_binary.h5', custom_objects={'L1Dist':L1Dist, 'BinaryCrossentropy':tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_input[2], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_val[2], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for 1 image\n",
    "y_hat = model.predict([np.expand_dims(test_input[1], axis=0), np.expand_dims(test_val[1], axis=0)])\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for all test images\n",
    "pr_arr = []\n",
    "f1_arr = []\n",
    "acc_arr = []\n",
    "\n",
    "for (test_input, test_val, y_true) in test_data.as_numpy_iterator():\n",
    "    y_hat = model.predict([test_input, test_val])\n",
    "\n",
    "    r = Recall()\n",
    "    r.update_state(y_true, y_hat)\n",
    "    recall_score = r.result().numpy()\n",
    "\n",
    "    p = Precision()\n",
    "    p.update_state(y_true, y_hat)\n",
    "    precision_score = p.result().numpy()\n",
    "\n",
    "    f1_score = (2 * precision_score * recall_score) / (precision_score + recall_score)\n",
    "\n",
    "    a = BinaryAccuracy()\n",
    "    a.update_state(y_true, y_hat)\n",
    "    accuracy_score = a.result().numpy()\n",
    "\n",
    "    # print(f'Recall: {recall_score}, Precision: {precision_score}')\n",
    "\n",
    "    pr_arr.append((recall_score, precision_score))\n",
    "    f1_arr.append(f1_score)\n",
    "    acc_arr.append(accuracy_score)\n",
    "\n",
    "f1_sum, acc_sum = 0, 0\n",
    "\n",
    "for i in range(len(f1_arr)):\n",
    "    f1_sum += f1_arr[i]\n",
    "    acc_sum += acc_arr[i]\n",
    "\n",
    "f1_avg = f1_sum / len(f1_arr)\n",
    "acc_avg = acc_sum / len(acc_arr)\n",
    "\n",
    "print(f'Avg Accuracy: {acc_avg}, Avg F1 Score: {f1_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_sum = 0\n",
    "# for i in range(len(f1_arr)-1):\n",
    "#     f1_sum += f1_arr[i]\n",
    "#     print(f1_arr[i])\n",
    "\n",
    "# print(f'Sum: {f1_sum}')\n",
    "# f1_avg = f1_sum / (len(f1_arr)-1)\n",
    "# print(f'Avg: {f1_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glaucoma_img = Image.open(r'D:\\Projects\\college\\Glaucoma Generation - Final Year\\fsl trial v2 - glaucoma\\data\\anchor\\POAG-000008-2009-02-03-OD.png')\n",
    "# glaucoma_np = np.asarray(glaucoma_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal_img = Image.open(r'D:\\Projects\\college\\Glaucoma Generation - Final Year\\fsl trial v2 - glaucoma\\data\\negative\\Normal-000002-2009-10-28-OD.png')\n",
    "# normal_np = np.asarray(normal_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(file_path):\n",
    "#     byte_img = tf.io.read_file(file_path)\n",
    "#     img = tf.io.decode_png(byte_img)    \n",
    "#     # img = tf.image.resize(img, (128,64))\n",
    "#     img = img / 255\n",
    "#     img = img[:,:,0]\n",
    "#     return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glaucoma_np = preprocess(r'D:\\Projects\\college\\Glaucoma Generation - Final Year\\fsl trial v2 - glaucoma\\data\\anchor\\POAG-000008-2009-02-03-OD.png')\n",
    "# normal_np = preprocess(r'D:\\Projects\\college\\Glaucoma Generation - Final Year\\fsl trial v2 - glaucoma\\data\\positive\\POAG-001815-2009-03-31-OD.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glaucoma_np = np.expand_dims(glaucoma_np, axis=0)\n",
    "# normal_np = np.expand_dims(normal_np, axis=0)\n",
    "# glaucoma_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(glaucoma_np.shape)\n",
    "# print(normal_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat = model.predict([glaucoma_np, normal_np])\n",
    "# y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1 if prediction > 0.5 else 0 for prediction in y_hat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fabd133e1875d3e858f66ae812b1afba53ab799979db7755b4d020e2a0b9328a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
