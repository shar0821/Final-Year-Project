{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPool2D, Input, Flatten, Dropout, GlobalAveragePooling2D, AveragePooling2D, Activation, BatchNormalization\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preventing tf from utilizing the full GPU\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting files from directories\n",
    "anchor = tf.data.Dataset.list_files(ANC_PATH+'\\*.png').take(262)\n",
    "positive = tf.data.Dataset.list_files(POS_PATH+'\\*.png').take(262)\n",
    "negative = tf.data.Dataset.list_files(NEG_PATH+'\\*.png').take(262)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_test = anchor.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dir_test.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def preprocess(file_path):\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_png(byte_img)    \n",
    "    # img = tf.image.resize(img, (128,64))\n",
    "    img = img / 255\n",
    "    img = img[:,:,0]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = preprocess(r\"data\\anchor\\POAG-000008-2009-02-03-OD.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img2 = img[:,:,0]\n",
    "# img2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img.numpy().max()\n",
    "# plt.imshow(img,cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating matching and non-matching pairs\n",
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = data.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper function for preprecessing 2 images\n",
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    return(preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = preprocess_twin(*example)\n",
    "# len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up data for training\n",
    "train_data = data.take(round(len(data)*.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up data for testing\n",
    "test_data = data.skip(round(len(data)*.7))\n",
    "test_data = test_data.take(round(len(data)*.3))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model - VGG16\n",
    "def make_embedding(): \n",
    "    inp = Input(shape=(128,64,1))\n",
    "\n",
    "    x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(inp)\n",
    "    x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "    x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "    x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units = 4096, activation ='relu')(x)\n",
    "    x = Dense(units = 4096, activation ='relu')(x)\n",
    "    output = Dense(units = 1, activation ='sigmoid')(x)\n",
    "\n",
    "    model = Model (inputs=inp, outputs =output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = make_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 64, 1)]      0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 128, 64, 64)       640       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 128, 64, 64)       36928     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 64, 32, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 64, 32, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 64, 32, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 32, 16, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 32, 16, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 32, 16, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 32, 16, 256)       590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 16, 8, 256)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 16, 8, 512)        1180160   \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 16, 8, 512)        2359808   \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 16, 8, 512)        2359808   \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 8, 4, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 8, 4, 512)         2359808   \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 8, 4, 512)         2359808   \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 8, 4, 512)         2359808   \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 4, 2, 512)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 4097      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48,280,257\n",
      "Trainable params: 48,280,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distancing/differencing layer\n",
    "class L1Dist(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, input_embedding, validation_embedding):\n",
    "        return tf.math.abs(input_embedding - validation_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = L1Dist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the siamese architecture\n",
    "def make_siamese_model(): \n",
    "    \n",
    "    input_image = Input(shape=(128,64,1))\n",
    "    \n",
    "    validation_image = Input(shape=(128,64,1))\n",
    "    \n",
    "    siamese_layer = L1Dist()\n",
    "    distances = siamese_layer(embedding(input_image), embedding(validation_image))\n",
    "    \n",
    "    classifier = Dense(1, activation='sigmoid')(distances)\n",
    "    \n",
    "    return Model(inputs=[input_image, validation_image], outputs=classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model = make_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 128, 64, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 128, 64, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " model (Functional)             (None, 1)            48280257    ['input_2[0][0]',                \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " l1_dist_1 (L1Dist)             (None, 1)            0           ['model[0][0]',                  \n",
      "                                                                  'model[1][0]']                  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            2           ['l1_dist_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 48,280,259\n",
      "Trainable params: 48,280,259\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up optimizer and loss\n",
    "binary_cross_loss = tf.losses.BinaryCrossentropy()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up checkpoints\n",
    "checkpoint_dir = './vgg-checkpoints/training_checkpoints_SGD_1e-2_200_vgg_custom'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=opt, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom function for each training step\n",
    "@tf.function\n",
    "def train_step(batch):\n",
    "    with tf.GradientTape() as tape:     \n",
    "        X = batch[:2]\n",
    "        y = batch[2]\n",
    "        \n",
    "        yhat = siamese_model(X, training=True)\n",
    "        loss = binary_cross_loss(y, yhat)\n",
    "    \n",
    "        \n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and saving checkpoints\n",
    "def train(data, EPOCHS):\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        print('\\n Epoch {}/{}'.format(epoch, EPOCHS))\n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        for idx, batch in enumerate(data):\n",
    "            train_step(batch)\n",
    "            progbar.update(idx+1)\n",
    "        \n",
    "        if epoch % 25 == 0: \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1/200\n",
      "Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "Tensor(\"binary_crossentropy/weighted_loss/value:0\", shape=(), dtype=float32)\n",
      "23/23 [==============================] - 17s 344ms/step\n",
      "\n",
      " Epoch 2/200\n",
      "23/23 [==============================] - 7s 285ms/step\n",
      "\n",
      " Epoch 3/200\n",
      "23/23 [==============================] - 7s 298ms/step\n",
      "\n",
      " Epoch 4/200\n",
      "23/23 [==============================] - 9s 372ms/step\n",
      "\n",
      " Epoch 5/200\n",
      "23/23 [==============================] - 10s 424ms/step\n",
      "\n",
      " Epoch 6/200\n",
      "23/23 [==============================] - 10s 417ms/step\n",
      "\n",
      " Epoch 7/200\n",
      "23/23 [==============================] - 10s 418ms/step\n",
      "\n",
      " Epoch 8/200\n",
      "23/23 [==============================] - 8s 355ms/step\n",
      "\n",
      " Epoch 9/200\n",
      "23/23 [==============================] - 8s 353ms/step\n",
      "\n",
      " Epoch 10/200\n",
      "23/23 [==============================] - 8s 353ms/step\n",
      "\n",
      " Epoch 11/200\n",
      "23/23 [==============================] - 8s 354ms/step\n",
      "\n",
      " Epoch 12/200\n",
      "23/23 [==============================] - 8s 359ms/step\n",
      "\n",
      " Epoch 13/200\n",
      "23/23 [==============================] - 8s 368ms/step\n",
      "\n",
      " Epoch 14/200\n",
      "23/23 [==============================] - 8s 363ms/step\n",
      "\n",
      " Epoch 15/200\n",
      "23/23 [==============================] - 8s 367ms/step\n",
      "\n",
      " Epoch 16/200\n",
      "23/23 [==============================] - 7s 323ms/step\n",
      "\n",
      " Epoch 17/200\n",
      "23/23 [==============================] - 8s 354ms/step\n",
      "\n",
      " Epoch 18/200\n",
      "23/23 [==============================] - 8s 353ms/step\n",
      "\n",
      " Epoch 19/200\n",
      "23/23 [==============================] - 8s 366ms/step\n",
      "\n",
      " Epoch 20/200\n",
      "23/23 [==============================] - 8s 360ms/step\n",
      "\n",
      " Epoch 21/200\n",
      "23/23 [==============================] - 9s 373ms/step\n",
      "\n",
      " Epoch 22/200\n",
      "23/23 [==============================] - 8s 362ms/step\n",
      "\n",
      " Epoch 23/200\n",
      "23/23 [==============================] - 8s 355ms/step\n",
      "\n",
      " Epoch 24/200\n",
      "23/23 [==============================] - 8s 364ms/step\n",
      "\n",
      " Epoch 25/200\n",
      "23/23 [==============================] - 8s 362ms/step\n",
      "\n",
      " Epoch 26/200\n",
      "23/23 [==============================] - 8s 358ms/step\n",
      "\n",
      " Epoch 27/200\n",
      "23/23 [==============================] - 7s 324ms/step\n",
      "\n",
      " Epoch 28/200\n",
      "23/23 [==============================] - 8s 354ms/step\n",
      "\n",
      " Epoch 29/200\n",
      "23/23 [==============================] - 8s 363ms/step\n",
      "\n",
      " Epoch 30/200\n",
      "23/23 [==============================] - 8s 353ms/step\n",
      "\n",
      " Epoch 31/200\n",
      "23/23 [==============================] - 8s 355ms/step\n",
      "\n",
      " Epoch 32/200\n",
      "23/23 [==============================] - 8s 355ms/step\n",
      "\n",
      " Epoch 33/200\n",
      "23/23 [==============================] - 8s 357ms/step\n",
      "\n",
      " Epoch 34/200\n",
      "23/23 [==============================] - 8s 354ms/step\n",
      "\n",
      " Epoch 35/200\n",
      "23/23 [==============================] - 8s 359ms/step\n",
      "\n",
      " Epoch 36/200\n",
      "23/23 [==============================] - 8s 365ms/step\n",
      "\n",
      " Epoch 37/200\n",
      "23/23 [==============================] - 8s 357ms/step\n",
      "\n",
      " Epoch 38/200\n",
      "23/23 [==============================] - 8s 360ms/step\n",
      "\n",
      " Epoch 39/200\n",
      "23/23 [==============================] - 8s 359ms/step\n",
      "\n",
      " Epoch 40/200\n",
      "23/23 [==============================] - 8s 362ms/step\n",
      "\n",
      " Epoch 41/200\n",
      "23/23 [==============================] - 8s 359ms/step\n",
      "\n",
      " Epoch 42/200\n",
      "23/23 [==============================] - 8s 360ms/step\n",
      "\n",
      " Epoch 43/200\n",
      "23/23 [==============================] - 8s 358ms/step\n",
      "\n",
      " Epoch 44/200\n",
      "23/23 [==============================] - 8s 358ms/step\n",
      "\n",
      " Epoch 45/200\n",
      "23/23 [==============================] - 9s 380ms/step\n",
      "\n",
      " Epoch 46/200\n",
      "23/23 [==============================] - 9s 406ms/step\n",
      "\n",
      " Epoch 47/200\n",
      "23/23 [==============================] - 11s 471ms/step\n",
      "\n",
      " Epoch 48/200\n",
      "23/23 [==============================] - 10s 438ms/step\n",
      "\n",
      " Epoch 49/200\n",
      "23/23 [==============================] - 8s 359ms/step\n",
      "\n",
      " Epoch 50/200\n",
      "23/23 [==============================] - 9s 372ms/step\n",
      "\n",
      " Epoch 51/200\n",
      "23/23 [==============================] - 8s 370ms/step\n",
      "\n",
      " Epoch 52/200\n",
      "23/23 [==============================] - 8s 358ms/step\n",
      "\n",
      " Epoch 53/200\n",
      "23/23 [==============================] - 8s 363ms/step\n",
      "\n",
      " Epoch 54/200\n",
      "23/23 [==============================] - 8s 358ms/step\n",
      "\n",
      " Epoch 55/200\n",
      "23/23 [==============================] - 20s 884ms/step\n",
      "\n",
      " Epoch 56/200\n",
      "23/23 [==============================] - 23s 1s/step\n",
      "\n",
      " Epoch 57/200\n",
      "23/23 [==============================] - 22s 963ms/step\n",
      "\n",
      " Epoch 58/200\n",
      "23/23 [==============================] - 21s 906ms/step\n",
      "\n",
      " Epoch 59/200\n",
      "23/23 [==============================] - 21s 905ms/step\n",
      "\n",
      " Epoch 60/200\n",
      "23/23 [==============================] - 20s 883ms/step\n",
      "\n",
      " Epoch 61/200\n",
      "23/23 [==============================] - 8s 356ms/step\n",
      "\n",
      " Epoch 62/200\n",
      "23/23 [==============================] - 8s 360ms/step\n",
      "\n",
      " Epoch 63/200\n",
      "23/23 [==============================] - 9s 384ms/step\n",
      "\n",
      " Epoch 64/200\n",
      "23/23 [==============================] - 8s 361ms/step\n",
      "\n",
      " Epoch 65/200\n",
      "23/23 [==============================] - 8s 370ms/step\n",
      "\n",
      " Epoch 66/200\n",
      "23/23 [==============================] - 9s 374ms/step\n",
      "\n",
      " Epoch 67/200\n",
      "23/23 [==============================] - 9s 375ms/step\n",
      "\n",
      " Epoch 68/200\n",
      "23/23 [==============================] - 9s 379ms/step\n",
      "\n",
      " Epoch 69/200\n",
      "23/23 [==============================] - 15s 658ms/step\n",
      "\n",
      " Epoch 70/200\n",
      "23/23 [==============================] - 21s 928ms/step\n",
      "\n",
      " Epoch 71/200\n",
      "23/23 [==============================] - 22s 968ms/step\n",
      "\n",
      " Epoch 72/200\n",
      "23/23 [==============================] - 23s 1s/step\n",
      "\n",
      " Epoch 73/200\n",
      "23/23 [==============================] - 25s 1s/step\n",
      "\n",
      " Epoch 74/200\n",
      "23/23 [==============================] - 24s 1s/step\n",
      "\n",
      " Epoch 75/200\n",
      "23/23 [==============================] - 21s 912ms/step\n",
      "\n",
      " Epoch 76/200\n",
      "23/23 [==============================] - 21s 927ms/step\n",
      "\n",
      " Epoch 77/200\n",
      "23/23 [==============================] - 21s 908ms/step\n",
      "\n",
      " Epoch 78/200\n",
      "23/23 [==============================] - 25s 1s/step\n",
      "\n",
      " Epoch 79/200\n",
      "23/23 [==============================] - 25s 1s/step\n",
      "\n",
      " Epoch 80/200\n",
      "23/23 [==============================] - 24s 1s/step\n",
      "\n",
      " Epoch 81/200\n",
      "23/23 [==============================] - 24s 1s/step\n",
      "\n",
      " Epoch 82/200\n",
      "23/23 [==============================] - 22s 967ms/step\n",
      "\n",
      " Epoch 83/200\n",
      "23/23 [==============================] - 24s 1s/step\n",
      "\n",
      " Epoch 84/200\n",
      "23/23 [==============================] - 22s 974ms/step\n",
      "\n",
      " Epoch 85/200\n",
      "23/23 [==============================] - 24s 1s/step\n",
      "\n",
      " Epoch 86/200\n",
      "23/23 [==============================] - 27s 1s/step\n",
      "\n",
      " Epoch 87/200\n",
      "23/23 [==============================] - 23s 999ms/step\n",
      "\n",
      " Epoch 88/200\n",
      "23/23 [==============================] - 22s 974ms/step\n",
      "\n",
      " Epoch 89/200\n",
      "23/23 [==============================] - 24s 1s/step\n",
      "\n",
      " Epoch 90/200\n",
      "23/23 [==============================] - 24s 1s/step\n",
      "\n",
      " Epoch 91/200\n",
      "23/23 [==============================] - 24s 1s/step\n",
      "\n",
      " Epoch 92/200\n",
      "23/23 [==============================] - 22s 980ms/step\n",
      "\n",
      " Epoch 93/200\n",
      "23/23 [==============================] - 23s 979ms/step\n",
      "\n",
      " Epoch 94/200\n",
      "23/23 [==============================] - 21s 928ms/step\n",
      "\n",
      " Epoch 95/200\n",
      "23/23 [==============================] - 21s 927ms/step\n",
      "\n",
      " Epoch 96/200\n",
      "23/23 [==============================] - 22s 952ms/step\n",
      "\n",
      " Epoch 97/200\n",
      "23/23 [==============================] - 21s 923ms/step\n",
      "\n",
      " Epoch 98/200\n",
      "23/23 [==============================] - 21s 918ms/step\n",
      "\n",
      " Epoch 99/200\n",
      "23/23 [==============================] - 21s 925ms/step\n",
      "\n",
      " Epoch 100/200\n",
      "23/23 [==============================] - 21s 909ms/step\n",
      "\n",
      " Epoch 101/200\n",
      "23/23 [==============================] - 20s 916ms/step\n",
      "\n",
      " Epoch 102/200\n",
      "23/23 [==============================] - 22s 950ms/step\n",
      "\n",
      " Epoch 103/200\n",
      "23/23 [==============================] - 21s 908ms/step\n",
      "\n",
      " Epoch 104/200\n",
      "23/23 [==============================] - 23s 1s/step\n",
      "\n",
      " Epoch 105/200\n",
      "23/23 [==============================] - 22s 937ms/step\n",
      "\n",
      " Epoch 106/200\n",
      "23/23 [==============================] - 22s 958ms/step\n",
      "\n",
      " Epoch 107/200\n",
      "23/23 [==============================] - 22s 956ms/step\n",
      "\n",
      " Epoch 108/200\n",
      "23/23 [==============================] - 22s 918ms/step\n",
      "\n",
      " Epoch 109/200\n",
      "23/23 [==============================] - 21s 918ms/step\n",
      "\n",
      " Epoch 110/200\n",
      "23/23 [==============================] - 22s 942ms/step\n",
      "\n",
      " Epoch 111/200\n",
      "23/23 [==============================] - 22s 969ms/step\n",
      "\n",
      " Epoch 112/200\n",
      "23/23 [==============================] - 25s 1s/step\n",
      "\n",
      " Epoch 113/200\n",
      "23/23 [==============================] - 21s 933ms/step\n",
      "\n",
      " Epoch 114/200\n",
      "23/23 [==============================] - 23s 1s/step\n",
      "\n",
      " Epoch 115/200\n",
      "23/23 [==============================] - 21s 915ms/step\n",
      "\n",
      " Epoch 116/200\n",
      "23/23 [==============================] - 21s 904ms/step\n",
      "\n",
      " Epoch 117/200\n",
      "23/23 [==============================] - 21s 908ms/step\n",
      "\n",
      " Epoch 118/200\n",
      "23/23 [==============================] - 22s 960ms/step\n",
      "\n",
      " Epoch 119/200\n",
      "23/23 [==============================] - 22s 946ms/step\n",
      "\n",
      " Epoch 120/200\n",
      "23/23 [==============================] - 22s 950ms/step\n",
      "\n",
      " Epoch 121/200\n",
      "23/23 [==============================] - 23s 988ms/step\n",
      "\n",
      " Epoch 122/200\n",
      "23/23 [==============================] - 22s 948ms/step\n",
      "\n",
      " Epoch 123/200\n",
      "23/23 [==============================] - 22s 956ms/step\n",
      "\n",
      " Epoch 124/200\n",
      "23/23 [==============================] - 22s 979ms/step\n",
      "\n",
      " Epoch 125/200\n",
      "23/23 [==============================] - 22s 955ms/step\n",
      "\n",
      " Epoch 126/200\n",
      "23/23 [==============================] - 20s 914ms/step\n",
      "\n",
      " Epoch 127/200\n",
      "23/23 [==============================] - 21s 931ms/step\n",
      "\n",
      " Epoch 128/200\n",
      "23/23 [==============================] - 23s 983ms/step\n",
      "\n",
      " Epoch 129/200\n",
      "23/23 [==============================] - 21s 904ms/step\n",
      "\n",
      " Epoch 130/200\n",
      "23/23 [==============================] - 21s 906ms/step\n",
      "\n",
      " Epoch 131/200\n",
      "23/23 [==============================] - 22s 944ms/step\n",
      "\n",
      " Epoch 132/200\n",
      "23/23 [==============================] - 24s 1s/step\n",
      "\n",
      " Epoch 133/200\n",
      "23/23 [==============================] - 21s 921ms/step\n",
      "\n",
      " Epoch 134/200\n",
      "23/23 [==============================] - 22s 970ms/step\n",
      "\n",
      " Epoch 135/200\n",
      "23/23 [==============================] - 23s 998ms/step\n",
      "\n",
      " Epoch 136/200\n",
      "23/23 [==============================] - 25s 1s/step\n",
      "\n",
      " Epoch 137/200\n",
      "23/23 [==============================] - 23s 999ms/step\n",
      "\n",
      " Epoch 138/200\n",
      "23/23 [==============================] - 23s 991ms/step\n",
      "\n",
      " Epoch 139/200\n",
      "23/23 [==============================] - 23s 1s/step\n",
      "\n",
      " Epoch 140/200\n",
      "23/23 [==============================] - 23s 997ms/step\n",
      "\n",
      " Epoch 141/200\n",
      "23/23 [==============================] - 22s 940ms/step\n",
      "\n",
      " Epoch 142/200\n",
      "23/23 [==============================] - 23s 976ms/step\n",
      "\n",
      " Epoch 143/200\n",
      "23/23 [==============================] - 23s 990ms/step\n",
      "\n",
      " Epoch 144/200\n",
      "23/23 [==============================] - 22s 982ms/step\n",
      "\n",
      " Epoch 145/200\n",
      "23/23 [==============================] - 22s 944ms/step\n",
      "\n",
      " Epoch 146/200\n",
      "23/23 [==============================] - 21s 916ms/step\n",
      "\n",
      " Epoch 147/200\n",
      "23/23 [==============================] - 21s 916ms/step\n",
      "\n",
      " Epoch 148/200\n",
      "23/23 [==============================] - 23s 992ms/step\n",
      "\n",
      " Epoch 149/200\n",
      "23/23 [==============================] - 21s 912ms/step\n",
      "\n",
      " Epoch 150/200\n",
      "23/23 [==============================] - 21s 905ms/step\n",
      "\n",
      " Epoch 151/200\n",
      "23/23 [==============================] - 20s 915ms/step\n",
      "\n",
      " Epoch 152/200\n",
      "23/23 [==============================] - 21s 928ms/step\n",
      "\n",
      " Epoch 153/200\n",
      "23/23 [==============================] - 21s 914ms/step\n",
      "\n",
      " Epoch 154/200\n",
      "23/23 [==============================] - 22s 970ms/step\n",
      "\n",
      " Epoch 155/200\n",
      "23/23 [==============================] - 22s 968ms/step\n",
      "\n",
      " Epoch 156/200\n",
      "23/23 [==============================] - 21s 918ms/step\n",
      "\n",
      " Epoch 157/200\n",
      "23/23 [==============================] - 21s 907ms/step\n",
      "\n",
      " Epoch 158/200\n",
      "23/23 [==============================] - 21s 911ms/step\n",
      "\n",
      " Epoch 159/200\n",
      "23/23 [==============================] - 21s 932ms/step\n",
      "\n",
      " Epoch 160/200\n",
      "23/23 [==============================] - 21s 914ms/step\n",
      "\n",
      " Epoch 161/200\n",
      "23/23 [==============================] - 21s 904ms/step\n",
      "\n",
      " Epoch 162/200\n",
      "23/23 [==============================] - 21s 910ms/step\n",
      "\n",
      " Epoch 163/200\n",
      "23/23 [==============================] - 21s 924ms/step\n",
      "\n",
      " Epoch 164/200\n",
      "23/23 [==============================] - 21s 916ms/step\n",
      "\n",
      " Epoch 165/200\n",
      "23/23 [==============================] - 22s 937ms/step\n",
      "\n",
      " Epoch 166/200\n",
      "23/23 [==============================] - 22s 949ms/step\n",
      "\n",
      " Epoch 167/200\n",
      "23/23 [==============================] - 22s 942ms/step\n",
      "\n",
      " Epoch 168/200\n",
      "23/23 [==============================] - 21s 909ms/step\n",
      "\n",
      " Epoch 169/200\n",
      "23/23 [==============================] - 21s 923ms/step\n",
      "\n",
      " Epoch 170/200\n",
      "23/23 [==============================] - 21s 927ms/step\n",
      "\n",
      " Epoch 171/200\n",
      "23/23 [==============================] - 24s 1s/step\n",
      "\n",
      " Epoch 172/200\n",
      "23/23 [==============================] - 21s 904ms/step\n",
      "\n",
      " Epoch 173/200\n",
      "23/23 [==============================] - 21s 904ms/step\n",
      "\n",
      " Epoch 174/200\n",
      "23/23 [==============================] - 21s 909ms/step\n",
      "\n",
      " Epoch 175/200\n",
      "23/23 [==============================] - 23s 994ms/step\n",
      "\n",
      " Epoch 176/200\n",
      "23/23 [==============================] - 22s 1s/step\n",
      "\n",
      " Epoch 177/200\n",
      "23/23 [==============================] - 21s 922ms/step\n",
      "\n",
      " Epoch 178/200\n",
      "23/23 [==============================] - 21s 917ms/step\n",
      "\n",
      " Epoch 179/200\n",
      "23/23 [==============================] - 22s 945ms/step\n",
      "\n",
      " Epoch 180/200\n",
      "23/23 [==============================] - 23s 1s/step\n",
      "\n",
      " Epoch 181/200\n",
      "23/23 [==============================] - 21s 905ms/step\n",
      "\n",
      " Epoch 182/200\n",
      "23/23 [==============================] - 22s 970ms/step\n",
      "\n",
      " Epoch 183/200\n",
      "23/23 [==============================] - 24s 1s/step\n",
      "\n",
      " Epoch 184/200\n",
      "23/23 [==============================] - 21s 912ms/step\n",
      "\n",
      " Epoch 185/200\n",
      "23/23 [==============================] - 21s 917ms/step\n",
      "\n",
      " Epoch 186/200\n",
      "23/23 [==============================] - 21s 928ms/step\n",
      "\n",
      " Epoch 187/200\n",
      "23/23 [==============================] - 21s 909ms/step\n",
      "\n",
      " Epoch 188/200\n",
      "23/23 [==============================] - 21s 909ms/step\n",
      "\n",
      " Epoch 189/200\n",
      "23/23 [==============================] - 21s 907ms/step\n",
      "\n",
      " Epoch 190/200\n",
      "23/23 [==============================] - 21s 932ms/step\n",
      "\n",
      " Epoch 191/200\n",
      "23/23 [==============================] - 22s 952ms/step\n",
      "\n",
      " Epoch 192/200\n",
      "23/23 [==============================] - 22s 946ms/step\n",
      "\n",
      " Epoch 193/200\n",
      "23/23 [==============================] - 21s 922ms/step\n",
      "\n",
      " Epoch 194/200\n",
      "23/23 [==============================] - 22s 965ms/step\n",
      "\n",
      " Epoch 195/200\n",
      "23/23 [==============================] - 21s 931ms/step\n",
      "\n",
      " Epoch 196/200\n",
      "23/23 [==============================] - 21s 925ms/step\n",
      "\n",
      " Epoch 197/200\n",
      "23/23 [==============================] - 22s 939ms/step\n",
      "\n",
      " Epoch 198/200\n",
      "23/23 [==============================] - 21s 921ms/step\n",
      "\n",
      " Epoch 199/200\n",
      "23/23 [==============================] - 21s 924ms/step\n",
      "\n",
      " Epoch 200/200\n",
      "23/23 [==============================] - 23s 1s/step\n"
     ]
    }
   ],
   "source": [
    "train(train_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# saving the model\n",
    "siamese_model.save('siamesemodel_vgg_custom_200_SGD_1e-2_binary.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_input, test_val, y_true = test_data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_hat = siamese_model.predict([test_input, test_val])\n",
    "# y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1 if prediction > 0.5 else 0 for prediction in y_hat ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = Recall()\n",
    "# m.update_state(y_true, y_hat)\n",
    "# m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = Precision()\n",
    "# m.update_state(y_true, y_hat)\n",
    "# m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,8))\n",
    "\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.imshow(test_input[0])\n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.imshow(test_val[0])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fabd133e1875d3e858f66ae812b1afba53ab799979db7755b4d020e2a0b9328a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
